{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Activation, Dropout, Lambda, Multiply, Add, Concatenate, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_module(inp_shape=(4096, 1)):\n",
    "    \n",
    "    # convolutional operation parameters\n",
    "    n_filters = 32 \n",
    "    filter_width = 2\n",
    "    dilation_rates = [2**i for i in range(11)] * 3 \n",
    "\n",
    "    # define an input history series and pass it through a stack of dilated causal convolution blocks. \n",
    "    Input_seq = Input(shape=inp_shape, dtype='float32')\n",
    "    x = Input_seq\n",
    "\n",
    "    skips = []\n",
    "    for dilation_rate in dilation_rates:\n",
    "\n",
    "        # preprocessing - equivalent to time-distributed dense\n",
    "        x = Conv1D(16, 1, padding='same', activation='relu')(x) \n",
    "\n",
    "        # filter convolution\n",
    "        x_f = Conv1D(filters=n_filters,\n",
    "                     kernel_size=filter_width, \n",
    "                     padding='same',\n",
    "                     dilation_rate=dilation_rate)(x)\n",
    "\n",
    "        # gating convolution\n",
    "        x_g = Conv1D(filters=n_filters,\n",
    "                     kernel_size=filter_width, \n",
    "                     padding='same',\n",
    "                     dilation_rate=dilation_rate)(x)\n",
    "\n",
    "        # multiply filter and gating branches\n",
    "        z = Multiply()([Activation('tanh')(x_f),\n",
    "                        Activation('sigmoid')(x_g)])\n",
    "\n",
    "        # postprocessing - equivalent to time-distributed dense\n",
    "        z = Conv1D(16, 1, padding='same', activation='relu')(z)\n",
    "\n",
    "        # residual connection\n",
    "        x = Add()([x, z])    \n",
    "sub_mod_A\n",
    "        # collect skip connections\n",
    "        skips.append(z)\n",
    "\n",
    "    # add all skip connection outputs \n",
    "    out = Activation('relu')(Add()(skips))\n",
    "    \n",
    "    sub_module = Model(Input_seq, out)\n",
    "    \n",
    "    return sub_module\n",
    "\n",
    "\n",
    "def full_module(inp_shape=(4096, 2)):\n",
    "    \n",
    "    # define an input history series and pass it through a stack of dilated causal convolution blocks. \n",
    "    Input_seq = Input(shape=inp_shape, dtype='float32')\n",
    "    \n",
    "    x_A = Lambda(lambda y: y[:,:,0])(Input_seq); x_A = Reshape(target_shape=(4096,1))(x_A)\n",
    "    x_B = Lambda(lambda y: y[:,:,1])(Input_seq); x_B = Reshape(target_shape=(4096,1))(x_B)\n",
    "    \n",
    "    sub_mod_A = sub_module(inp_shape=(4096, 1))\n",
    "    sub_mod_B = sub_module(inp_shape=(4096, 1))\n",
    "    \n",
    "    x_A = sub_mod_A(x_A)\n",
    "    x_B = sub_mod_B(x_B)\n",
    "    \n",
    "    out = tf.keras.layers.Concatenate(axis=-1)([x_A, x_B])\n",
    "    'sigmoid'\n",
    "    # final time-distributed dense layers \n",
    "    out = Conv1D(128, 1, padding='same')(out)\n",
    "    out = Activation('relu')(out)\n",
    "    #out = Dropout(.2)(out)\n",
    "    out = Conv1D(1, 1, padding='same')(out)\n",
    "    \n",
    "    #out = Reshape((8192,))(out)\n",
    "    out = Activation('sigmoid')(out)\n",
    "    \n",
    "    model = Model(Input_seq, out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = full_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4096, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 4096, 1)      0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 4096, 1)      0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "functional_1 (Functional)       (None, 4096, 16)     95856       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "functional_3 (Functional)       (None, 4096, 16)     95856       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4096, 32)     0           functional_1[0][0]               \n",
      "                                                                 functional_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_264 (Conv1D)             (None, 4096, 128)    4224        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 4096, 128)    0           conv1d_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_265 (Conv1D)             (None, 4096, 1)      129         activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 4096, 1)      0           conv1d_265[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 196,065\n",
      "Trainable params: 196,065\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "ERROR: Unsupported layer type: Lambda",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6d019ef230b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_from_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Precision'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ap_fixed<16,6>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ReuseFactor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/hls4ml/utils/config.py\u001b[0m in \u001b[0;36mconfig_from_keras_model\u001b[0;34m(model, granularity, default_precision, default_reuse_factor)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkeras_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeras_layer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeras_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupported_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR: Unsupported layer type: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeras_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: ERROR: Unsupported layer type: Lambda"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import hls4ml\n",
    "# #import plotting\n",
    "# import matplotlib.pyplot as plt\n",
    "# import yaml\n",
    "# config = hls4ml.utils.config_from_keras_model(M, granularity='name')\n",
    "# config['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "# config['Model']['ReuseFactor'] = 1\n",
    "# for i in config['LayerName']:\n",
    "#     config['LayerName'][i]['ReuseFactor'] = 100\n",
    "# config['Model']['Strategy'] = 'Resource'\n",
    "# #config['LayerName']['soft']['Precision'] = 'ap_fixed<20,8>'\n",
    "# #config['LayerName']['dense1']['Strategy'] = 'Resource'\n",
    "# config['LayerName']['soft']['Strategy'] = 'Stable'\n",
    "#plotting.print_dict(config)\n",
    "# hls_model = hls4ml.converters.convert_from_keras_model(model,\n",
    "#                                                        output_dir='resnet50/hls4ml_prj',\n",
    "#                                                        fpga_part='xcu250-figd2104-2L-e',\n",
    "#                                                        io_type = 'io_stream',\n",
    "#                                                        hls_config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
